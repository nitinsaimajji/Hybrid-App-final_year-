{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1600638b-09e0-4164-9165-a06808d7ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('final_train.csv')\n",
    "test_data = pd.read_csv('final_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ecf0f9-a91c-401e-8aec-1c40e84f2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# Pad sequences\n",
    "maxlen = 100  # adjust as needed\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\n",
    "    'bully-Spam': 0,\n",
    "    'not_bully-Spam': 1,\n",
    "    'bully-Ham': 2,\n",
    "    'not_bully-Ham': 3\n",
    "}\n",
    "y_train = train_data['new_label'].map(label_mapping)\n",
    "y_test = test_data['new_label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd088d6d-618b-4e88-b741-450bed086332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "40/40 [==============================] - 10s 124ms/step - loss: 1.1394 - accuracy: 0.4385 - val_loss: 0.7982 - val_accuracy: 0.8085\n",
      "Epoch 2/15\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.8275 - accuracy: 0.5837 - val_loss: 0.9460 - val_accuracy: 0.3546\n",
      "Epoch 3/15\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.6960 - accuracy: 0.7534 - val_loss: 0.5748 - val_accuracy: 0.9574\n",
      "Epoch 4/15\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.4795 - accuracy: 0.8779 - val_loss: 0.8528 - val_accuracy: 0.6454\n",
      "Epoch 5/15\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.2677 - accuracy: 0.9302 - val_loss: 0.7471 - val_accuracy: 0.7021\n",
      "Epoch 6/15\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.1893 - accuracy: 0.9485 - val_loss: 0.7303 - val_accuracy: 0.7447\n",
      "Epoch 7/15\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.1389 - accuracy: 0.9635 - val_loss: 0.6968 - val_accuracy: 0.7376\n",
      "Epoch 8/15\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.0890 - accuracy: 0.9778 - val_loss: 0.9337 - val_accuracy: 0.6809\n",
      "Epoch 9/15\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0791 - accuracy: 0.9810 - val_loss: 0.5097 - val_accuracy: 0.8723\n",
      "Epoch 10/15\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0481 - accuracy: 0.9873 - val_loss: 0.9788 - val_accuracy: 0.6950\n",
      "Epoch 11/15\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.0357 - accuracy: 0.9913 - val_loss: 0.8538 - val_accuracy: 0.7518\n",
      "Epoch 12/15\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0283 - accuracy: 0.9929 - val_loss: 0.8927 - val_accuracy: 0.7447\n",
      "Epoch 13/15\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0200 - accuracy: 0.9968 - val_loss: 0.7230 - val_accuracy: 0.7872\n",
      "Epoch 14/15\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0161 - accuracy: 0.9992 - val_loss: 1.0166 - val_accuracy: 0.7234\n",
      "Epoch 15/15\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.8626 - val_accuracy: 0.7447\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "# Build Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Bidirectional LSTM model\n",
    "history=model.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88921293-17a9-45e3-bf44-3ef397528956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 27ms/step\n",
      "Bi-LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        10\n",
      "           1       0.89      0.81      0.85       115\n",
      "           2       0.81      0.85      0.83       102\n",
      "           3       0.84      0.84      0.84       199\n",
      "\n",
      "    accuracy                           0.84       426\n",
      "   macro avg       0.81      0.88      0.84       426\n",
      "weighted avg       0.84      0.84      0.84       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Bi-LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test)\n",
    "y_pred = predictions.argmax(axis=1)\n",
    "print(\"Bi-LSTM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1ad69e-56e3-44e5-aaa6-f6e34c5f9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model(\"final_bilstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47395fff-dbf2-493d-8997-de9ab7e069ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "label_mapping = {\n",
    "    'bully-Spam': 0,\n",
    "    'not_bully-Spam': 1,\n",
    "    'bully-Ham': 2,\n",
    "    'not_bully-Ham': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a856def-fabb-4b7b-b69f-d9d8fad36fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted label for the new text: not_bully-Ham\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess new text\n",
    "new_text = \"@harryc0422 thatâ€™s not true at all. I am a Chinese and most urban places in Zhejiang and Jiangsu have better infrastructure than US. I feel my life quality really sucks in the USâ€¦and I come from a small place within Ningbo, Zhejiang\"\n",
    "new_text_sequence = tokenizer.texts_to_sequences([new_text])\n",
    "new_text_padded = pad_sequences(new_text_sequence, padding='post', maxlen=100)\n",
    "\n",
    "# Make predictions on new text\n",
    "new_text_prediction = saved_model.predict(new_text_padded)\n",
    "predicted_label = np.argmax(new_text_prediction)\n",
    "\n",
    "# Map predicted label back to original label\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "predicted_label_text = reverse_label_mapping[predicted_label]\n",
    "\n",
    "print(\"Predicted label for the new text:\", predicted_label_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1d57c-2419-48a5-957e-5daf52067302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de92777-fada-45da-addf-e751e9897ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('final_train.csv')\n",
    "test_data = pd.read_csv('final_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0093e1-2ebf-4033-a398-312d4346c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# Pad sequences\n",
    "maxlen = 100  # adjust as needed\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\n",
    "    'bully-Spam': 0,\n",
    "    'not_bully-Spam': 1,\n",
    "    'bully-Ham': 2,\n",
    "    'not_bully-Ham': 3\n",
    "}\n",
    "y_train = train_data['new_label'].map(label_mapping)\n",
    "y_test = test_data['new_label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7290f73-3277-4c6a-b06f-ed59516af25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\nitin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adadelta.py:79: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\nitin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\nadam.py:86: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 25ms/step\n",
      "Report for rmsprop\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.30      0.46        10\n",
      "           1       0.91      0.83      0.87       115\n",
      "           2       0.80      0.85      0.82       102\n",
      "           3       0.83      0.86      0.85       199\n",
      "\n",
      "    accuracy                           0.84       426\n",
      "   macro avg       0.88      0.71      0.75       426\n",
      "weighted avg       0.85      0.84      0.84       426\n",
      "\n",
      "14/14 [==============================] - 2s 25ms/step\n",
      "Report for adadelta\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33        10\n",
      "           1       0.91      0.78      0.84       115\n",
      "           2       0.73      0.81      0.77       102\n",
      "           3       0.82      0.87      0.85       199\n",
      "\n",
      "    accuracy                           0.82       426\n",
      "   macro avg       0.87      0.67      0.70       426\n",
      "weighted avg       0.83      0.82      0.82       426\n",
      "\n",
      "14/14 [==============================] - 2s 51ms/step\n",
      "Report for nadam\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.90      0.75        10\n",
      "           1       0.97      0.82      0.89       115\n",
      "           2       0.77      0.83      0.80       102\n",
      "           3       0.83      0.85      0.84       199\n",
      "\n",
      "    accuracy                           0.84       426\n",
      "   macro avg       0.80      0.85      0.82       426\n",
      "weighted avg       0.85      0.84      0.84       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Nadam\n",
    "import pandas as pd\n",
    "\n",
    "# Define a list of optimizers to try along with their learning rates\n",
    "optimizers = {\n",
    "    'rmsprop': RMSprop(lr=0.001),\n",
    "    'adadelta': Adadelta(lr=1.0),\n",
    "    'nadam': Nadam(lr=0.002)\n",
    "}\n",
    "\n",
    "\n",
    "# Loop over each optimizer\n",
    "for optimizer_name in optimizers:\n",
    "    # Build DNN model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer=optimizers[optimizer_name], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(\"Report for\",optimizer_name)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb4a24-cb6a-4300-bffa-3d6f6dee3de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
