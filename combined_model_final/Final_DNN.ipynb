{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600638b-09e0-4164-9165-a06808d7ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('final_train.csv')\n",
    "test_data = pd.read_csv('final_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd99ab6-44f0-48e3-ab4f-3edc02916735",
   "metadata": {},
   "source": [
    "## build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68153442-5205-4749-ae83-accf2c9f3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check if GPU is available and set memory growth to avoid memory allocation issues\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('final_train.csv')\n",
    "test_data = pd.read_csv('final_test.csv')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_length = 128  # Reduced max token length\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['label'] = label_encoder.fit_transform(train_data['new_label'])\n",
    "test_data['label'] = label_encoder.transform(test_data['new_label'])\n",
    "\n",
    "# Function to encode texts\n",
    "def encode_texts(texts):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts.tolist(),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Encode the texts\n",
    "train_encodings = encode_texts(train_data['text'])\n",
    "test_encodings = encode_texts(test_data['text'])\n",
    "\n",
    "# Extract labels\n",
    "train_labels = train_data['label'].values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Load the DistilBERT model\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the DNN for classification\n",
    "input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "cls_output = bert_outputs.last_hidden_state[:, 0, :]  # Take the CLS token output\n",
    "\n",
    "# Add DNN layers\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(cls_output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(x)  # Use the number of unique classes\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Convert the data into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).shuffle(1000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model with XLA (Accelerated Linear Algebra) optimization\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64638bcb-c29d-4c4b-8f5e-08e82bb62ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    inputs, labels = batch\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred_batch = model.predict(inputs)\n",
    "    y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93176e81-3075-4dd9-90dc-63f7b817543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae828b8-2f68-431d-9a00-3ee666c43929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fe3fd-6910-4b89-925d-18b300d4738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326bc3e-6119-4cfb-909d-c7432720b6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcc394-ddeb-49a3-bc64-f647a458c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
