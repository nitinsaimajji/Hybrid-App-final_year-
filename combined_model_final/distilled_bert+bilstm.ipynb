{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11068be9-9938-42f2-9357-2d0d61d4bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79/79 [==============================] - 200s 2s/step - loss: 0.7686 - accuracy: 0.6868 - val_loss: 0.6311 - val_accuracy: 0.7376\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 187s 2s/step - loss: 0.3718 - accuracy: 0.8699 - val_loss: 0.3119 - val_accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 188s 2s/step - loss: 0.2185 - accuracy: 0.9278 - val_loss: 0.1734 - val_accuracy: 0.9433\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 190s 2s/step - loss: 0.1262 - accuracy: 0.9651 - val_loss: 0.1175 - val_accuracy: 0.9504\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 199s 3s/step - loss: 0.0788 - accuracy: 0.9738 - val_loss: 0.1803 - val_accuracy: 0.9220\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('final_train.csv')\n",
    "test_data = pd.read_csv('final_test.csv')\n",
    "\n",
    "# Tokenize text data using DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 100  # Maximum sequence length\n",
    "X_train_text = tokenizer(train_data['text'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
    "X_test_text = tokenizer(test_data['text'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {'bully-Spam': 0, 'not_bully-Spam': 1, 'bully-Ham': 2, 'not_bully-Ham': 3}\n",
    "y_train = train_data['new_label'].map(label_mapping)\n",
    "y_test = test_data['new_label'].map(label_mapping)\n",
    "\n",
    "# Define DistilBERT model\n",
    "def build_distil_bert_model():\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    distil_bert_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    distil_bert_outputs = distil_bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    return Model(inputs=[input_ids, attention_mask], outputs=distil_bert_outputs)\n",
    "\n",
    "# Build DistilBERT model\n",
    "distil_bert_model = build_distil_bert_model()\n",
    "\n",
    "# Combine DistilBERT and Bi-LSTM models\n",
    "def build_combined_model(distil_bert_model):\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    distil_bert_output = distil_bert_model([input_ids, attention_mask])\n",
    "    \n",
    "    # Add Bi-LSTM layer\n",
    "    bi_lstm_output = Bidirectional(LSTM(128))(distil_bert_output)\n",
    "    \n",
    "    output = Dense(4, activation='softmax')(bi_lstm_output)\n",
    "\n",
    "    return Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Build combined model\n",
    "combined_model = build_combined_model(distil_bert_model)\n",
    "\n",
    "# Compile combined model with a smaller learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Adjust the learning rate as needed\n",
    "combined_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train combined model with a smaller batch size\n",
    "history = combined_model.fit(\n",
    "    [X_train_text['input_ids'], X_train_text['attention_mask']], \n",
    "    y_train, \n",
    "    epochs=5, \n",
    "    batch_size=16, \n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27118628-de7c-490b-842a-7be70a5bf73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 18s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.95      0.91      0.93       115\n",
      "           2       0.89      0.83      0.86       103\n",
      "           3       0.87      0.91      0.89       198\n",
      "\n",
      "    accuracy                           0.90       426\n",
      "   macro avg       0.93      0.92      0.92       426\n",
      "weighted avg       0.90      0.90      0.90       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "predictions = combined_model.predict([X_test_text['input_ids'], X_test_text['attention_mask']])\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert test labels to numpy array\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_np, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf9136-b5cb-4d04-90d9-411b635d06c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
